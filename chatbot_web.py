import os
import re
import json
import torch
import uvicorn
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_huggingface import HuggingFaceEndpoint, HuggingFacePipeline
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

app = FastAPI()

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins in development
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ì§€ì •
# ìˆ˜ì • í›„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
STATIC_DIR = BASE_DIR 

# PDF íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° íŒŒì¼ ë°ì´í„° êµ¬ì„±
pdf_paths = [
    "ì •ì±…ê³µì•½ì§‘.pdf",
    "ì§€ì—­ê³µì•½.pdf"
]

# PDF íŒŒì¼ ì •ë³´ - API ìš”ì²­ ì‹œ ë°˜í™˜ë  ì •ë³´
pdf_files_info = {
    "full": {
        "title": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹ ì œ20ëŒ€ ëŒ€í†µë ¹ì„ ê±° ì •ì±…ê³µì•½ì§‘",
        "path": "/static/pdfs/full.pdf",
        "thumbnail": "/static/images/pdf_thumbnail.jpg"
    },
    "file1": {
        "title": "ì‚¶ì˜ í„°ì „ë³„ ê³µì•½",
        "path": "/static/pdfs/file1.pdf"
    },
    "file2": {
        "title": "ëŒ€ìƒë³„ ê³µì•½",
        "path": "/static/pdfs/file2.pdf"
    },
    "file3": {
        "title": "1. ì‹ ê²½ì œ",
        "path": "/static/pdfs/file3.pdf"
    },
    "file4": {
        "title": "2. ê³µì •ì„±ì¥",
        "path": "/static/pdfs/file4.pdf"
    },
    "file5": {
        "title": "3. ë¯¼ìƒì•ˆì •",
        "path": "/static/pdfs/file5.pdf"
    },
    "file6": {
        "title": "4. ë¯¼ì£¼ì‚¬íšŒ",
        "path": "/static/pdfs/file6.pdf"
    },
    "file7": {
        "title": "5. í‰í™”ì•ˆë³´",
        "path": "/static/pdfs/file7.pdf"
    },
    "file8": {
        "title": "ì†Œí™•í–‰Â·ëª…í™•í–‰Â·SNSë°œí‘œ ê³µì•½",
        "path": "/static/pdfs/file8.pdf"
    }
}

# Serve static files (HTML, CSS, JS)
app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")

# ì „ì—­ ë³€ìˆ˜ë¡œ LangChain êµ¬ì„±ìš”ì†Œ ì„ ì–¸
retriever = None
llm = None
rag_chain = None

# 1. ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ ì¿¼ë¦¬ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸
query_transformation_prompt = PromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ë¬¸ì„œ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , ê´€ë ¨ ë¬¸ì„œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ 
    ìµœì ì˜ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
    
    ì›ë˜ ì§ˆë¬¸: {question}
    
    ê²€ìƒ‰ì— ì‚¬ìš©í•  í‚¤ì›Œë“œë‚˜ ë¬¸êµ¬ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.
    """
)

# 2. ë¬¸ì„œ ë¶„ì„ í”„ë¡¬í”„íŠ¸
document_analysis_prompt = PromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì •ì±… ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ë¶„ì„í•˜ê³ , ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ì±…ë“¤ì„ ì°¾ì•„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
    
    ì§ˆë¬¸: {question}
    
    ë¬¸ì„œ ë‚´ìš©:
    {context}
    
    ìœ„ ë¬¸ì„œì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê°€ì¥ ì¤‘ìš”í•œ ì •ì±…/ê³µì•½ì„ ìµœëŒ€ 10ê°œê¹Œì§€ ì¶”ì¶œí•˜ì—¬ 1,000ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
    ê° ì •ì±…ì€ í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
    ë˜í•œ ë§ˆì§€ë§‰ì— ì¶œì²˜ê°€ ë˜ëŠ” ê³µì•½ì˜ page ì™€ ë¬¸ì„œëª…ì„ ë°˜ë“œì‹œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    ê´€ë ¨ ì •ì±…ì´ ì—†ë‹¤ë©´ "ê´€ë ¨ ì •ì±… ì •ë³´ ì—†ìŒ"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.
    """
)

'''
ìœ„ ë¬¸ì„œì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ì±…/ê³µì•½ë§Œ ì¶”ì¶œí•˜ì„¸ìš”. 
ê° ì •ì±…/ê³µì•½ì˜ í•µì‹¬ ë‚´ìš©ê³¼ ìœ„ì¹˜í•œ ë¬¸ì„œ ë¶€ë¶„ì„ ëª…ì‹œí•˜ì„¸ìš”.
ê´€ë ¨ ì •ì±…ì´ ì—†ë‹¤ë©´ "ê´€ë ¨ ì •ì±… ì •ë³´ ì—†ìŒ"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.
'''


# ì „ì²´ ë©€í‹°ëª¨ë‹¬ RAG ì²´ì¸ êµ¬ì„±
def create_multimodal_rag_chain(retriever, llm):
    # 1. ì¿¼ë¦¬ ë³€í™˜ ì²´ì¸
    query_transformer_chain = (
        {"question": RunnablePassthrough()}
        | query_transformation_prompt
        | llm
        | StrOutputParser()
    )
    
    # 2. ê²€ìƒ‰ ì²´ì¸ (ë³€í™˜ëœ ì¿¼ë¦¬ ì‚¬ìš©)
    def retrieve_documents(input_dict):
        original_question = input_dict["original_question"]
        optimized_query = input_dict["optimized_query"]
        
        print(f"ì›ë³¸ ì§ˆë¬¸: {original_question}")
        print(f"ìµœì í™”ëœ ì¿¼ë¦¬: {optimized_query}")
        
        # ìµœì í™”ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        retrieved_docs = retriever.invoke(optimized_query)
        
        print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}")
        if retrieved_docs:
            print(f"ì²« ë²ˆì§¸ ë¬¸ì„œ ì¼ë¶€: {retrieved_docs[0].page_content[:100]}...")
        
        return {
            "question": original_question,
            "context": "\n\n".join([doc.page_content for doc in retrieved_docs])
        }
    
    # 3. ë¬¸ì„œ ë¶„ì„ ì²´ì¸
    document_analysis_chain = (
        document_analysis_prompt
        | llm
        | StrOutputParser()
    )
    
    # ì „ì²´ ë©€í‹°ëª¨ë‹¬ ì²´ì¸ êµ¬ì„±
    multimodal_chain = (
        # 1ë‹¨ê³„: ì›ë³¸ ì§ˆë¬¸ ì €ì¥ ë° ì¿¼ë¦¬ ìµœì í™”
        RunnablePassthrough().with_config(run_name="Original Question")
        | {"original_question": lambda x: x, "optimized_query": query_transformer_chain}
        
        # 2ë‹¨ê³„: ìµœì í™”ëœ ì¿¼ë¦¬ë¡œ ë¬¸ì„œ ê²€ìƒ‰
        | RunnableLambda(retrieve_documents).with_config(run_name="Document Retrieval")
        
        # 3ë‹¨ê³„: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¶„ì„
        | {"question": lambda x: x["question"], 
           "analyzed_info": document_analysis_chain}
        
        # 4ë‹¨ê³„: ìµœì¢… ì‘ë‹µ ìƒì„± (ë¶„ì„ëœ ì •ë³´ë¥¼ ì‚¬ìš©ì ì¹œí™”ì  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜)
        | RunnableLambda(lambda x: format_response(x["question"], x["analyzed_info"]))
    )
    
    return multimodal_chain

# format_response í•¨ìˆ˜ì— ë¡œê·¸ ì¶”ê°€
def format_response(question, analyzed_info):
    """ì‘ë‹µì„ ì‚¬ìš©ì ì¹œí™”ì ì¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    print(f"ì§ˆë¬¸: {question}")
    print(f"ë¶„ì„ëœ ì •ë³´: {analyzed_info}")
    
    # analyzed_infoì—ì„œ ì •ì±… ì •ë³´ ì¶”ì¶œ
    policies = []
    
    # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¡œ ì •ì±… í•­ëª© ì¶”ì¶œ
    info_lines = analyzed_info.split('\n')
    current_policy = None
    
    for line in info_lines:
        line = line.strip()
        if not line:
            continue
            
        # ìƒˆ ì •ì±… í•­ëª© ì‹œì‘ìœ¼ë¡œ ë³´ì´ëŠ” íŒ¨í„´
        if re.match(r'^[0-9]+\.', line) or re.match(r'^â€¢', line) or re.match(r'^-', line):
            if current_policy:
                policies.append(current_policy)
            current_policy = line
        elif current_policy:
            current_policy += " " + line
    
    # ë§ˆì§€ë§‰ ì •ì±… ì¶”ê°€
    if current_policy:
        policies.append(current_policy)
    
    print(f"ì¶”ì¶œëœ ì •ì±… ìˆ˜: {len(policies)}")
    if len(policies) > 0:
        print(f"ì²« ë²ˆì§¸ ì •ì±…: {policies[0]}")
    
    else:
        print("ê´€ë ¨ ì •ì±… ì •ë³´ê°€ ì—†ìŒ")
        return f"ğŸ¤– {question} ê´€ë ¨ ë‹µë³€ë“œë¦½ë‹ˆë‹¤.\n\nì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­í•˜ì‹  '{question}'ì— ê´€í•œ ì •ì±… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ìœ¼ë¡œ ì‹œë„í•´ ë³´ì„¸ìš”."
    
    # ì‘ë‹µ êµ¬ì„±
    response = f"ğŸ¤– {question} ê´€ë ¨ ë‹µë³€ë“œë¦½ë‹ˆë‹¤.\n\n"
    for i, policy in enumerate(policies, 1):  # ìµœëŒ€ 3ê°œ ì •ì±…ë§Œ í‘œì‹œ
        response += f"{i}. {policy}\n\n"
    
    return response

# init_rag_system í•¨ìˆ˜ì— ë¡œê·¸ ì¶”ê°€
def init_rag_system():
    """LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global retriever, llm, rag_chain

    print("LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

    # 1. PDF ë¬¸ì„œ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
    documents = []
    for pdf_path in pdf_paths:
        if os.path.exists(pdf_path):
            print(f"PDF íŒŒì¼ ë¡œë“œ ì¤‘: {pdf_path}")
            loader = PyPDFLoader(pdf_path)
            docs = loader.load()
            documents.extend(docs)
            print(f"- {len(docs)}ê°œ í˜ì´ì§€ ë¡œë“œë¨")
        else:
            print(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_path}")

    if not documents:
        print("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False
    
    print(f"ì´ {len(documents)}ê°œ í˜ì´ì§€ ë¡œë“œë¨")

    # 2. í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_documents(documents)
    print(f"ë¬¸ì„œë¥¼ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    # 3. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    print("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/distiluse-base-multilingual-cased-v1",
        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    print("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")

    # 4. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
    print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...")
    vectorstore = FAISS.from_documents(chunks, embedding_model)
    print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ")

    # ìƒ˜í”Œ ë¬¸ì„œ í™•ì¸ (ë””ë²„ê¹…ìš©)
    for i, doc_id in enumerate(list(vectorstore.docstore._dict.keys())[:3]):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
        print(f"ë¬¸ì„œ ID {doc_id}ì˜ ë‚´ìš©:")
        print(vectorstore.docstore._dict[doc_id])
        print("-" * 50)
        if i >= 2:  # ìµœëŒ€ 3ê°œë§Œ ì¶œë ¥
            break

    # 5. ê²€ìƒ‰ê¸°(Retriever) ì„¤ì • - ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì§ì ‘ ìƒì„±
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 5}
    )

    # 6. EXAONE ëª¨ë¸ ë¡œë“œ ë° LangChain LLM ë˜í¼ ì„¤ì •
    model_name = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"
    print(f"{model_name} ëª¨ë¸ ë¡œë“œ ì¤‘...")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )

    # ì–‘ìí™” ì„¤ì •ì„ BitsAndBytesConfigë¡œ êµ¬ì„±
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True
    )

    # ëª¨ë¸ ë¡œë“œ - ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )

    # ëª¨ë¸ ìµœì í™” ì„¤ì • ì ìš©
    optimize_performance(model)

    # ì§ì ‘ Hugging Face íŒŒì´í”„ë¼ì¸ ìƒì„± í›„ LangChain ë˜í¼ ì ìš©
    from transformers import pipeline

    # íŠ¸ëœìŠ¤í¬ë¨¸ íŒŒì´í”„ë¼ì¸ ìƒì„±
    hf_pipeline = pipeline(
        task="text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=200,
        do_sample=True,
        temperature=0.3,
        device_map="auto",
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id
    )

    # LangChain HuggingFacePipeline ë˜í¼ ìƒì„±
    llm = HuggingFacePipeline(pipeline=hf_pipeline)

    print("EXAONE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    rag_chain = create_multimodal_rag_chain(retriever, llm)

    print("LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    return True

def optimize_performance(model):
    """ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì ìš©"""
    # GPU ë©”ëª¨ë¦¬ ìµœì í™”
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.cuda.empty_cache()
        torch.cuda.set_device(torch.cuda.current_device())

    # ëª¨ë¸ ìµœì í™”
    if hasattr(model, 'config'):
        if hasattr(model.config, 'use_cache'):
            model.config.use_cache = True
        if hasattr(model.config, 'gradient_checkpointing'):
            model.config.gradient_checkpointing = False

    print("ì„±ëŠ¥ ìµœì í™” ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ê¸°ë³¸ ê²½ë¡œ - ë©”ì¸ HTML íŒŒì¼ ë°˜í™˜
@app.get('/')
async def root():
    return FileResponse(os.path.join(STATIC_DIR, "index.html"))

# PDF í˜ì´ì§€ ë¼ìš°íŠ¸ ì¶”ê°€
@app.get('/pdf')
async def pdf_page():
    return FileResponse(os.path.join(STATIC_DIR, "pdf.html"))

# PDF íŒŒì¼ ì •ë³´ API ì—”ë“œí¬ì¸íŠ¸
@app.get('/api/files')
async def get_files():
    return JSONResponse(pdf_files_info)

# ì›¹ ì±„íŒ… API ì—”ë“œí¬ì¸íŠ¸
@app.post('/api/chat')
async def chat_endpoint(request: Request):
    global rag_chain
    
    # ìš”ì²­ ë°”ë”” íŒŒì‹±
    req_data = await request.json()
    user_message = req_data.get('message', '')
    
    if not user_message:
        return JSONResponse({"response": "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."})
    
    try:
        # RAG ì²´ì¸ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        response = rag_chain.invoke(user_message)
        
        # í´ë¼ì´ì–¸íŠ¸ ì‘ë‹µ í˜•ì‹
        return JSONResponse({
            "response": response
        })
    except Exception as e:
        print(f"Error processing request: {str(e)}")
        return JSONResponse({
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        })

# ì„œë²„ ì´ˆê¸°í™” ë° ì‹¤í–‰ì„ ìœ„í•œ ì´ë²¤íŠ¸
@app.on_event("startup")
async def startup_event():
    # ëª¨ë¸ ë° RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    init_success = init_rag_system()

    if not init_success:
        print("ì´ˆê¸°í™” ì‹¤íŒ¨. ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        import sys
        sys.exit(1)
    else:
        print("ì„œë²„ ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ! ì›¹ ì±—ë´‡ ì„œë²„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")

# ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    # 80ë²ˆ í¬íŠ¸ ì‚¬ìš© (root ê¶Œí•œ í•„ìš”)
    #uvicorn.run("chatbot_web:app", host="0.0.0.0", port=80, reload=True)
    
    # ê°œë°œìš©ìœ¼ë¡œëŠ” 5000ë²ˆ í¬íŠ¸ ì‚¬ìš©
    uvicorn.run("chatbot_web:app", host="0.0.0.0", port=5000,http="auto", reload=True)
    # HTTPSë¡œ ì‹¤í–‰
    #uvicorn.run(
    #    "chatbot_web:app", 
    #    host="0.0.0.0", 
    #    port=5000, 
    #    ssl_keyfile=os.path.join(BASE_DIR, "ca/private.pem"),  # ê°œì¸ í‚¤ ê²½ë¡œ 
    #    ssl_certfile=os.path.join(BASE_DIR, "ca/private_ct.pem"),  # ì¸ì¦ì„œ ê²½ë¡œ
    #    reload=True
    #)
