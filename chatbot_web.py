import logging
import os
from logging.handlers import RotatingFileHandler
from datetime import datetime
import re
import json
import torch
import uvicorn
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_huggingface import HuggingFaceEndpoint, HuggingFacePipeline
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

app = FastAPI()

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins in development
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ì§€ì •
# ìˆ˜ì • í›„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
STATIC_DIR = BASE_DIR 

# PDF íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° íŒŒì¼ ë°ì´í„° êµ¬ì„±
pdf_paths = [
    "ì •ì±…ê³µì•½ì§‘.pdf",
    "ì§€ì—­ê³µì•½.pdf"
]

# PDF íŒŒì¼ ì •ë³´ - API ìš”ì²­ ì‹œ ë°˜í™˜ë  ì •ë³´
pdf_files_info = {
    "full": {
        "title": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹ ì œ20ëŒ€ ëŒ€í†µë ¹ì„ ê±° ì •ì±…ê³µì•½ì§‘",
        "path": "/static/pdfs/full.pdf",
        "thumbnail": "/static/images/pdf_thumbnail.jpg"
    },
    "file1": {
        "title": "ì‚¶ì˜ í„°ì „ë³„ ê³µì•½",
        "path": "/static/pdfs/file1.pdf"
    },
    "file2": {
        "title": "ëŒ€ìƒë³„ ê³µì•½",
        "path": "/static/pdfs/file2.pdf"
    },
    "file3": {
        "title": "1. ì‹ ê²½ì œ",
        "path": "/static/pdfs/file3.pdf"
    },
    "file4": {
        "title": "2. ê³µì •ì„±ì¥",
        "path": "/static/pdfs/file4.pdf"
    },
    "file5": {
        "title": "3. ë¯¼ìƒì•ˆì •",
        "path": "/static/pdfs/file5.pdf"
    },
    "file6": {
        "title": "4. ë¯¼ì£¼ì‚¬íšŒ",
        "path": "/static/pdfs/file6.pdf"
    },
    "file7": {
        "title": "5. í‰í™”ì•ˆë³´",
        "path": "/static/pdfs/file7.pdf"
    },
    "file8": {
        "title": "ì†Œí™•í–‰Â·ëª…í™•í–‰Â·SNSë°œí‘œ ê³µì•½",
        "path": "/static/pdfs/file8.pdf"
    }
}

# Serve static files (HTML, CSS, JS)
app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")

# ì „ì—­ ë³€ìˆ˜ë¡œ LangChain êµ¬ì„±ìš”ì†Œ ì„ ì–¸
retriever = None
llm = None
rag_chain = None

# 1. ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ ì¿¼ë¦¬ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸
query_transformation_prompt = PromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ë¬¸ì„œ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , ê´€ë ¨ ë¬¸ì„œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ 
    ìµœì ì˜ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
    
    ì›ë˜ ì§ˆë¬¸: {question}
    
    ê²€ìƒ‰ì— ì‚¬ìš©í•  í‚¤ì›Œë“œë‚˜ ë¬¸êµ¬ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.
    """
)

# 2. ë¬¸ì„œ ë¶„ì„ í”„ë¡¬í”„íŠ¸
document_analysis_prompt = PromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì •ì±… ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ë¶„ì„í•˜ê³ , ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ì±…ë“¤ì„ ì°¾ì•„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
    
    ì§ˆë¬¸: {question}
    
    ë¬¸ì„œ ë‚´ìš©:
    {context}
    
    ìœ„ ë¬¸ì„œì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê°€ì¥ ì¤‘ìš”í•œ ì •ì±…/ê³µì•½ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
    ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”:
    1. ì •ì±…/ê³µì•½ì€ ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ì¶”ì¶œí•˜ì„¸ìš”. 5ê°œë¥¼ ë„˜ì§€ ë§ˆì„¸ìš”.
    2. ê° ì •ì±…ì€ í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ 100ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
    3. ì „ì²´ ì‘ë‹µì€ 1,000ìë¥¼ ë„˜ì§€ ì•Šë„ë¡ í•˜ì„¸ìš”.
    4. ê° ì •ì±…ì€ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ êµ¬ë¶„í•˜ê³ , ì •ì±…ë³„ë¡œ í•œ ì¤„ì”© ë„ì›Œì£¼ì„¸ìš”.
    5. ë§ˆì§€ë§‰ì— ì¶œì²˜ê°€ ë˜ëŠ” ê³µì•½ì˜ pageì™€ ë¬¸ì„œëª…ì„ ë°˜ë“œì‹œ í‘œì‹œí•˜ì„¸ìš”.
    6. ê´€ë ¨ ì •ì±…ì´ ì—†ë‹¤ë©´ "ê´€ë ¨ ì •ì±… ì •ë³´ ì—†ìŒ"ì´ë¼ê³ ë§Œ ë‹µí•˜ì„¸ìš”.
    """
)

# ê°ì • ë¶„ì„ í”„ë¡¬í”„íŠ¸
sentiment_analysis_prompt = PromptTemplate.from_template(
    """ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ê³¼ ì˜ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
    
    í…ìŠ¤íŠ¸: {text}
    
    ì´ í…ìŠ¤íŠ¸ê°€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¼ë„ ëª…í™•í•˜ê²Œ í¬í•¨í•˜ëŠ” ê²½ìš°ì—ë§Œ "ë¶€ì ì ˆ"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”:
    1. ì§ì ‘ì ì¸ ë¹„ì†ì–´ë‚˜ ìš•ì„¤
    2. ëª…ë°±íˆ ì„±ì ìœ¼ë¡œ ë¶€ì ì ˆí•œ ë‚´ìš©
    3. íŠ¹ì • ì§‘ë‹¨ì„ í–¥í•œ í˜ì˜¤ í‘œí˜„ì´ë‚˜ ì°¨ë³„ì  ì–¸ì–´
    4. ì§ì ‘ì ì¸ ìœ„í˜‘ì´ë‚˜ í­ë ¥ì ì¸ ë‚´ìš©
    5. ê°œì¸ì •ë³´ ìš”ì²­ì´ë‚˜ ìœ ì¶œ
    6. ëª…ë°±íˆ ë¶ˆë²•ì ì¸ í™œë™ ìœ ë„
    7. íŠ¹ì • ì •ì¹˜ì¸ì´ë‚˜ ê°œì¸ì— ëŒ€í•œ ì‹¬í•œ ë¹„ë°©ì´ë‚˜ ì¸ì‹ ê³µê²©
    
    ì¼ë°˜ì ì¸ ì§ˆë¬¸, ì •ì±… ë¬¸ì˜, ì¤‘ë¦½ì  ì˜ê²¬, ë‹¨ìˆœí•œ ë¶€ì •ì  ì˜ê²¬ í‘œí˜„ ë“±ì€ "ì ì ˆ"ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    ì •ì¹˜ì  ì£¼ì œë‚˜ ë¹„íŒì  ì§ˆë¬¸ì´ë¼ë„ ì˜ˆì˜ë¥¼ ê°–ì¶”ê³  ìˆë‹¤ë©´ "ì ì ˆ"ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    ì˜ë„ê°€ ë¶ˆë¶„ëª…í•˜ê±°ë‚˜ ë§¥ë½ì´ ë¶ˆì¶©ë¶„í•˜ë‹¤ë©´ "ì ì ˆ"ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    
    ê²°ê³¼(ì •í™•íˆ "ì ì ˆ" ë˜ëŠ” "ë¶€ì ì ˆ" ì¤‘ í•˜ë‚˜ë§Œ ë‹µë³€):
    """
)

# ë¡œê¹… ì„¤ì • í•¨ìˆ˜
def setup_logging():
    """ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = os.path.join(BASE_DIR, "logs")
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    # ë¡œê·¸ íŒŒì¼ ì´ë¦„ (ë‚ ì§œ í¬í•¨)
    log_filename = os.path.join(log_dir, f"chatbot_{datetime.now().strftime('%Y-%m-%d')}.log")
    
    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger("chatbot")
    logger.setLevel(logging.INFO)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ ì„¤ì • (10MB í¬ê¸° ì œí•œ, ìµœëŒ€ 5ê°œ ë°±ì—… íŒŒì¼)
    file_handler = RotatingFileHandler(
        log_filename, 
        maxBytes=10*1024*1024,  # 10 MB
        backupCount=5,
        encoding='utf-8'
    )
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬ ì„¤ì •
    console_handler = logging.StreamHandler()
    
    # í¬ë§· ì„¤ì •
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    # í•¸ë“¤ëŸ¬ ì¶”ê°€
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# ê°ì • ë¶„ì„ ì²´ì¸ ìƒì„±
def create_sentiment_analysis_chain(llm):
    sentiment_chain = (
        sentiment_analysis_prompt
        | llm
        | StrOutputParser()
    )
    return sentiment_chain

# ê°ì • ë¶„ì„ í•¨ìˆ˜
async def analyze_sentiment(text, sentiment_chain):
    """í…ìŠ¤íŠ¸ì˜ ê°ì •ê³¼ ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆì„± íŒë‹¨"""
    try:
        # ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        result = sentiment_chain.invoke({"text": text})
        
        # ë¡œê·¸ì— ë¶„ì„ ê²°ê³¼ ê¸°ë¡
        logger.info(f"ê°ì • ë¶„ì„ ì›ë³¸ ê²°ê³¼: '{result}' (í…ìŠ¤íŠ¸: '{text[:50]}...')")
        
        # ë‹¨ìˆœí™”ëœ ê²°ê³¼ ì¶”ì¶œ: "ì ì ˆ" ë˜ëŠ” "ë¶€ì ì ˆ" í‚¤ì›Œë“œë¥¼ ì°¾ìŒ
        is_inappropriate = False
        
        # ê²°ê³¼ì—ì„œ ë§ˆì§€ë§‰ 50ìë§Œ ê²€ì‚¬ (ìµœì¢… íŒë‹¨ì€ ë³´í†µ ë§ˆì§€ë§‰ì— ìˆìŒ)
        last_part = result[-50:] if len(result) > 50 else result
        
        if "ë¶€ì ì ˆ" in last_part:
            is_inappropriate = True
        elif "ì ì ˆ" in last_part:
            is_inappropriate = False
        else:
            # ë‘ í‚¤ì›Œë“œê°€ ëª¨ë‘ ì—†ìœ¼ë©´ ë¶„ì„ ì‹¤íŒ¨ë¡œ ê°„ì£¼, ê¸°ë³¸ê°’ ì‚¬ìš©
            logger.warning(f"ê°ì • ë¶„ì„ ê²°ê³¼ì—ì„œ íŒë‹¨ í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {last_part}")
            is_inappropriate = False
        
        logger.info(f"ë¶€ì ì ˆ ì—¬ë¶€ íŒë‹¨: {is_inappropriate}")
        
        return is_inappropriate, result
    except Exception as e:
        logger.error(f"ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False, f"ì˜¤ë¥˜: {str(e)}"
    
# ê¸ˆì§€ì–´ ëª©ë¡
prohibited_words = [
    "ì”¨ë°œ", "ë³‘ì‹ ", "ê°œìƒˆë¼", "ì§€ë„", "ì¢†", "ë‹ˆë¯¸", "fuck", "sex", "bastard", "bitch",
    "ê°œìì‹", "ê±¸ë ˆ", "ì°½ë…€", "ìŒë†ˆ", "ìŒë…„", "ì• ë¯¸", "ì• ë¹„", 
    "ì „í™”ë²ˆí˜¸", "ì£¼ë¯¼ë²ˆí˜¸", "ê³„ì¢Œë²ˆí˜¸", "ì‹ ìš©ì¹´ë“œ", "í´ë¼ìš°ë“œ", "cloud"
]

def contains_prohibited_content(text):
    """ì…ë ¥ëœ í…ìŠ¤íŠ¸ì— ë¹„ì†ì–´ë‚˜ ê°œì¸ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    text_lower = text.lower()
    
    # ê¸ˆì§€ì–´ ì²´í¬
    for word in prohibited_words:
        if word.lower() in text_lower:
            return True
            
    # ì´ë¦„ íŒ¨í„´ ì²´í¬
    name_pattern = re.compile(r'[ê°€-í£]{2,4}\s?ì”¨')
    if name_pattern.search(text):
        return True
        
    return False

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ ë”•ì…”ë„ˆë¦¬
conversation_history = {}

# ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ë©”ì‹œì§€ ì¶”ê°€ í•¨ìˆ˜
def add_to_history(session_id, role, content):
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ë©”ì‹œì§€ ì¶”ê°€"""
    if session_id not in conversation_history:
        conversation_history[session_id] = []
    
    conversation_history[session_id].append({
        "role": role,
        "content": content,
        "timestamp": datetime.now().isoformat()  # datetime ì§ì ‘ ì‚¬ìš©
    })
    
    # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ (ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€)
    if len(conversation_history[session_id]) > 10:
        conversation_history[session_id] = conversation_history[session_id][-10:]

# ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
def get_conversation_context(session_id):
    if session_id not in conversation_history or len(conversation_history[session_id]) == 0:
        return ""
    
    context = "ì´ì „ ëŒ€í™” ë‚´ìš©:\n"
    for message in conversation_history[session_id][-3:]:  # ìµœê·¼ 3ê°œ ë©”ì‹œì§€ë§Œ ì‚¬ìš©
        role_text = "ì‚¬ìš©ì" if message["role"] == "user" else "ì±—ë´‡"
        context += f"{role_text}: {message['content']}\n"
    
    return context + "\n"

# ì „ì²´ ë©€í‹°ëª¨ë‹¬ RAG ì²´ì¸ êµ¬ì„±
def create_multimodal_rag_chain(retriever, llm):
    # 1. ì¿¼ë¦¬ ë³€í™˜ ì²´ì¸
    query_transformer_chain = (
        {"question": RunnablePassthrough()}
        | query_transformation_prompt
        | llm
        | StrOutputParser()
    )
    
    # 2. ê²€ìƒ‰ ì²´ì¸ (ë³€í™˜ëœ ì¿¼ë¦¬ ì‚¬ìš©)
    def retrieve_documents(input_dict):
        original_question = input_dict["original_question"]
        optimized_query = input_dict["optimized_query"]
        
        logger.info(f"ì›ë³¸ ì§ˆë¬¸: {original_question}")
        logger.info(f"ìµœì í™”ëœ ì¿¼ë¦¬: {optimized_query}")
        
        # ìµœì í™”ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        retrieved_docs = retriever.invoke(optimized_query)
        
        logger.info(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}")
        if retrieved_docs:
            logger.info(f"ì²« ë²ˆì§¸ ë¬¸ì„œ ì¼ë¶€: {retrieved_docs[0].page_content[:100]}...")
        
        return {
            "question": original_question,
            "context": "\n\n".join([doc.page_content for doc in retrieved_docs])
        }
    
    # 3. ë¬¸ì„œ ë¶„ì„ ì²´ì¸
    document_analysis_chain = (
        document_analysis_prompt
        | llm
        | StrOutputParser()
    )
    
    # ì „ì²´ ë©€í‹°ëª¨ë‹¬ ì²´ì¸ êµ¬ì„±
    multimodal_chain = (
        # 1ë‹¨ê³„: ì›ë³¸ ì§ˆë¬¸ ì €ì¥ ë° ì¿¼ë¦¬ ìµœì í™”
        RunnablePassthrough().with_config(run_name="Original Question")
        | {"original_question": lambda x: x, "optimized_query": query_transformer_chain}
        
        # 2ë‹¨ê³„: ìµœì í™”ëœ ì¿¼ë¦¬ë¡œ ë¬¸ì„œ ê²€ìƒ‰
        | RunnableLambda(retrieve_documents).with_config(run_name="Document Retrieval")
        
        # 3ë‹¨ê³„: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¶„ì„
        | {"question": lambda x: x["question"], 
           "analyzed_info": document_analysis_chain}
        
        # 4ë‹¨ê³„: ìµœì¢… ì‘ë‹µ ìƒì„± (ë¶„ì„ëœ ì •ë³´ë¥¼ ì‚¬ìš©ì ì¹œí™”ì  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜)
        | RunnableLambda(lambda x: format_response(x["question"], x["analyzed_info"]))
    )
    
    return multimodal_chain

# format_response í•¨ìˆ˜ì— ë¡œê·¸ ì¶”ê°€
def format_response(question, analyzed_info):
    """ì‘ë‹µì„ ì‚¬ìš©ì ì¹œí™”ì ì¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    logger.info(f"ì§ˆë¬¸: {question}")
    logger.info(f"ë¶„ì„ëœ ì •ë³´: {analyzed_info}")
    
    # analyzed_infoì—ì„œ ì •ì±… ì •ë³´ ì¶”ì¶œ
    policies = []
    
    # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¡œ ì •ì±… í•­ëª© ì¶”ì¶œ
    info_lines = analyzed_info.split('\n')
    current_policy = None
    
    for line in info_lines:
        line = line.strip()
        if not line:
            continue
            
        # ìƒˆ ì •ì±… í•­ëª© ì‹œì‘ìœ¼ë¡œ ë³´ì´ëŠ” íŒ¨í„´
        if re.match(r'^[0-9]+\.', line) or re.match(r'^â€¢', line) or re.match(r'^-', line):
            if current_policy:
                policies.append(current_policy)
            current_policy = line
        elif current_policy:
            current_policy += " " + line
    
    # ë§ˆì§€ë§‰ ì •ì±… ì¶”ê°€
    if current_policy:
        policies.append(current_policy)
    
    logger.info(f"ì¶”ì¶œëœ ì •ì±… ìˆ˜: {len(policies)}")
    if len(policies) > 0:
        logger.info(f"ì²« ë²ˆì§¸ ì •ì±…: {policies[0]}")
    
    else:
        logger.info("ê´€ë ¨ ì •ì±… ì •ë³´ê°€ ì—†ìŒ")
        return f"ğŸ¤– {question} ê´€ë ¨ ë‹µë³€ë“œë¦½ë‹ˆë‹¤.\n\nì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­í•˜ì‹  '{question}'ì— ê´€í•œ ì •ì±… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ìœ¼ë¡œ ì‹œë„í•´ ë³´ì„¸ìš”."
    
    # ì‘ë‹µ êµ¬ì„±
    response = f"ğŸ¤– {question} ê´€ë ¨ ë‹µë³€ë“œë¦½ë‹ˆë‹¤.\n\n"
    for i, policy in enumerate(policies, 1):  # ìµœëŒ€ 3ê°œ ì •ì±…ë§Œ í‘œì‹œ
        response += f"{i}. {policy}\n\n"
    
    return response

# init_rag_system í•¨ìˆ˜ì— ë¡œê·¸ ì¶”ê°€
def init_rag_system():
    """LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global retriever, llm, rag_chain ,sentiment_chain

    logger.info("LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

    # 1. PDF ë¬¸ì„œ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
    documents = []
    for pdf_path in pdf_paths:
        if os.path.exists(pdf_path):
            logger.info(f"PDF íŒŒì¼ ë¡œë“œ ì¤‘: {pdf_path}")
            loader = PyPDFLoader(pdf_path)
            docs = loader.load()
            documents.extend(docs)
            logger.info(f"- {len(docs)}ê°œ í˜ì´ì§€ ë¡œë“œë¨")
        else:
            logger.info(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_path}")

    if not documents:
        logger.info("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False
    
    logger.info(f"ì´ {len(documents)}ê°œ í˜ì´ì§€ ë¡œë“œë¨")

    # 2. í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_documents(documents)
    logger.info(f"ë¬¸ì„œë¥¼ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    # 3. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/distiluse-base-multilingual-cased-v1",
        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")

    # 4. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
    logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...")
    vectorstore = FAISS.from_documents(chunks, embedding_model)
    logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ")

    # ìƒ˜í”Œ ë¬¸ì„œ í™•ì¸ (ë””ë²„ê¹…ìš©)
    for i, doc_id in enumerate(list(vectorstore.docstore._dict.keys())[:3]):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
        logger.info(f"ë¬¸ì„œ ID {doc_id}ì˜ ë‚´ìš©:")
        logger.info(vectorstore.docstore._dict[doc_id])
        logger.info("-" * 50)
        if i >= 2:  # ìµœëŒ€ 3ê°œë§Œ ì¶œë ¥
            break

    # 5. ê²€ìƒ‰ê¸°(Retriever) ì„¤ì • - ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì§ì ‘ ìƒì„±
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 5}
    )

    # 6. EXAONE ëª¨ë¸ ë¡œë“œ ë° LangChain LLM ë˜í¼ ì„¤ì •
    model_name = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"
    logger.info(f"{model_name} ëª¨ë¸ ë¡œë“œ ì¤‘...")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )

    # ì–‘ìí™” ì„¤ì •ì„ BitsAndBytesConfigë¡œ êµ¬ì„±
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True
    )

    # ëª¨ë¸ ë¡œë“œ - ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )

    # ëª¨ë¸ ìµœì í™” ì„¤ì • ì ìš©
    optimize_performance(model)

    # ì§ì ‘ Hugging Face íŒŒì´í”„ë¼ì¸ ìƒì„± í›„ LangChain ë˜í¼ ì ìš©
    from transformers import pipeline

    # íŠ¸ëœìŠ¤í¬ë¨¸ íŒŒì´í”„ë¼ì¸ ìƒì„±
    hf_pipeline = pipeline(
        task="text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=200,
        do_sample=True,
        temperature=0.3,
        device_map="auto",
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id
    )

    # LangChain HuggingFacePipeline ë˜í¼ ìƒì„±
    llm = HuggingFacePipeline(pipeline=hf_pipeline)

    logger.info("EXAONE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    rag_chain = create_multimodal_rag_chain(retriever, llm)

    # ê°ì • ë¶„ì„ ì²´ì¸ ìƒì„±
    sentiment_chain = create_sentiment_analysis_chain(llm)

    logger.info("LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    return True

def optimize_performance(model):
    """ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì ìš©"""
    # GPU ë©”ëª¨ë¦¬ ìµœì í™”
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.cuda.empty_cache()
        torch.cuda.set_device(torch.cuda.current_device())

    # ëª¨ë¸ ìµœì í™”
    if hasattr(model, 'config'):
        if hasattr(model.config, 'use_cache'):
            model.config.use_cache = True
        if hasattr(model.config, 'gradient_checkpointing'):
            model.config.gradient_checkpointing = False

    logger.info("ì„±ëŠ¥ ìµœì í™” ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ê¸°ë³¸ ê²½ë¡œ - ë©”ì¸ HTML íŒŒì¼ ë°˜í™˜
@app.get('/')
async def root():
    return FileResponse(os.path.join(STATIC_DIR, "index.html"))

# PDF í˜ì´ì§€ ë¼ìš°íŠ¸ ì¶”ê°€
@app.get('/pdf')
async def pdf_page():
    return FileResponse(os.path.join(STATIC_DIR, "pdf.html"))

# PDF íŒŒì¼ ì •ë³´ API ì—”ë“œí¬ì¸íŠ¸
@app.get('/api/files')
async def get_files():
    return JSONResponse(pdf_files_info)

# ì›¹ ì±„íŒ… API ì—”ë“œí¬ì¸íŠ¸
@app.post('/api/chat')
async def chat_endpoint(request: Request):
    global rag_chain, sentiment_chain
    
    # ìš”ì²­ ë°”ë”” íŒŒì‹±
    req_data = await request.json()
    user_message = req_data.get('message', '')
    session_id = req_data.get('session_id', 'default')
    debug_mode = req_data.get('debug_mode', False)  # ë””ë²„ê·¸ ëª¨ë“œ í”Œë˜ê·¸ ì¶”ê°€
    
    logger.info(f"ì„¸ì…˜ {session_id}ì—ì„œ ìƒˆë¡œìš´ ë©”ì‹œì§€ ìˆ˜ì‹ : {user_message[:30]}..." if len(user_message) > 30 else user_message)
    
    if not user_message:
        logger.warning("ë¹ˆ ë©”ì‹œì§€ê°€ ì „ì†¡ë¨")
        return JSONResponse({"response": "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."})
    
    try:
        sentiment_debug_info = {}  # ê°ì • ë¶„ì„ ë””ë²„ê·¸ ì •ë³´
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ê¸ˆì§€ì–´ í•„í„°ë§
        if contains_prohibited_content(user_message):
            logger.warning(f"ê¸ˆì§€ì–´ í•„í„°ë§ - ë¶€ì ì ˆí•œ ë‚´ìš© ê°ì§€: {user_message[:30]}...")
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. ë¶€ì ì ˆí•œ ì–¸ì–´ë‚˜ ê°œì¸ì •ë³´ê°€ í¬í•¨ëœ ì§ˆë¬¸ì—ëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            sentiment_debug_info["filter_type"] = "prohibited_words"
            
            if debug_mode:
                return JSONResponse({
                    "response": response,
                    "debug_info": sentiment_debug_info
                })
            return JSONResponse({"response": response})
        
        # 2ë‹¨ê³„: ê°ì • ë¶„ì„ì„ í†µí•œ ë¶€ì •ì  ì–´íœ˜ í•„í„°ë§
        is_inappropriate, analysis_result = await analyze_sentiment(user_message, sentiment_chain)
        
        # ë””ë²„ê·¸ ì •ë³´ ì €ì¥
        sentiment_debug_info = {
            "analysis_result": analysis_result,
            "is_inappropriate": is_inappropriate
        }
        
        if is_inappropriate:
            logger.warning(f"ê°ì • ë¶„ì„ í•„í„°ë§ - ë¶€ì •ì  ë‚´ìš© ê°ì§€: {user_message[:30]}...")
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. ë¶€ì ì ˆí•˜ê±°ë‚˜ ë¶€ì •ì ì¸ ë‚´ìš©ì´ í¬í•¨ëœ ì§ˆë¬¸ì—ëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            sentiment_debug_info["filter_type"] = "sentiment_analysis"
            
            if debug_mode:
                return JSONResponse({
                    "response": response,
                    "debug_info": sentiment_debug_info
                })
            return JSONResponse({"response": response})
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        add_to_history(session_id, "user", user_message)
        
        # ì´ì „ ëŒ€í™” ë‚´ìš© ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        context = get_conversation_context(session_id)
        
        # ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ RAG ì²´ì¸ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        logger.info("RAG ì²´ì¸ìœ¼ë¡œ ì‘ë‹µ ìƒì„± ì¤‘...")
        start_time = datetime.now()
        
        contextual_message = f"{context}ìƒˆë¡œìš´ ì§ˆë¬¸: {user_message}"
        response = rag_chain.invoke(contextual_message)
        
        processing_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"ì‘ë‹µ ìƒì„± ì™„ë£Œ (ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ)")
        
        # ì±—ë´‡ ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        add_to_history(session_id, "assistant", response)
        
        if debug_mode:
            return JSONResponse({
                "response": response,
                "session_id": session_id,
                "debug_info": sentiment_debug_info,
                "processing_time_seconds": processing_time
            })
        
        return JSONResponse({
            "response": response,
            "session_id": session_id
        })
    except Exception as e:
        logger.error(f"ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        return JSONResponse({
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "session_id": session_id,
            "error": str(e) if debug_mode else None
        })

# ì„œë²„ ì´ˆê¸°í™” ë° ì‹¤í–‰ì„ ìœ„í•œ ì´ë²¤íŠ¸
@app.on_event("startup")
async def startup_event():
    global logger
    
    # ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •
    logger = setup_logging()
    logger.info("==== ì›¹ ì±—ë´‡ ì„œë²„ ì‹œì‘ ====")
    
    # ëª¨ë¸ ë° RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    logger.info("LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘")
    init_success = init_rag_system()

    if not init_success:
        logger.error("ì´ˆê¸°í™” ì‹¤íŒ¨. ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        import sys
        sys.exit(1)
    else:
        logger.info("ì„œë²„ ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ! ì›¹ ì±—ë´‡ ì„œë²„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
  

# ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    # 80ë²ˆ í¬íŠ¸ ì‚¬ìš© (root ê¶Œí•œ í•„ìš”)
    #uvicorn.run("chatbot_web:app", host="0.0.0.0", port=80, reload=True)
    
    # ê°œë°œìš©ìœ¼ë¡œëŠ” 5000ë²ˆ í¬íŠ¸ ì‚¬ìš©
    uvicorn.run("chatbot_web:app", host="0.0.0.0", port=5000,http="auto", reload=True)
    # HTTPSë¡œ ì‹¤í–‰
    #uvicorn.run(
    #    "chatbot_web:app", 
    #    host="0.0.0.0", 
    #    port=5000, 
    #    ssl_keyfile=os.path.join(BASE_DIR, "ca/private.pem"),  # ê°œì¸ í‚¤ ê²½ë¡œ 
    #    ssl_certfile=os.path.join(BASE_DIR, "ca/private_ct.pem"),  # ì¸ì¦ì„œ ê²½ë¡œ
    #    reload=True
    #)
