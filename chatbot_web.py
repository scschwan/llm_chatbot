import logging
import os
from logging.handlers import RotatingFileHandler
from datetime import datetime
import re
import json
import csv
import uuid
import torch
import uvicorn
from pydantic import BaseModel
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_huggingface import HuggingFaceEndpoint, HuggingFacePipeline
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

app = FastAPI()

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins in development
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ì§€ì •
# ìˆ˜ì • í›„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
STATIC_DIR = BASE_DIR 
CSV_FILE = os.path.join(BASE_DIR, "comments.csv")

if not os.path.exists(CSV_FILE):
    with open(CSV_FILE, "w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["id", "text", "timestamp"])

# PDF íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° íŒŒì¼ ë°ì´í„° êµ¬ì„±
pdf_paths = [
    "ì •ì±…ê³µì•½ì§‘.pdf",
    "ì§€ì—­ê³µì•½.pdf"
]

# PDF íŒŒì¼ ì •ë³´ - API ìš”ì²­ ì‹œ ë°˜í™˜ë  ì •ë³´
pdf_files_info = {
    "full": {
        "title": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹ ì œ20ëŒ€ ëŒ€í†µë ¹ì„ ê±° ì •ì±…ê³µì•½ì§‘",
        "path": "/static/pdfs/full.pdf",
        "thumbnail": "/static/images/pdf_thumbnail.jpg"
    },
    "file1": {
        "title": "ì‚¶ì˜ í„°ì „ë³„ ê³µì•½",
        "path": "/static/pdfs/file1.pdf"
    },
    "file2": {
        "title": "ëŒ€ìƒë³„ ê³µì•½",
        "path": "/static/pdfs/file2.pdf"
    },
    "file3": {
        "title": "1. ì‹ ê²½ì œ",
        "path": "/static/pdfs/file3.pdf"
    },
    "file4": {
        "title": "2. ê³µì •ì„±ì¥",
        "path": "/static/pdfs/file4.pdf"
    },
    "file5": {
        "title": "3. ë¯¼ìƒì•ˆì •",
        "path": "/static/pdfs/file5.pdf"
    },
    "file6": {
        "title": "4. ë¯¼ì£¼ì‚¬íšŒ",
        "path": "/static/pdfs/file6.pdf"
    },
    "file7": {
        "title": "5. í‰í™”ì•ˆë³´",
        "path": "/static/pdfs/file7.pdf"
    },
    "file8": {
        "title": "ì†Œí™•í–‰Â·ëª…í™•í–‰Â·SNSë°œí‘œ ê³µì•½",
        "path": "/static/pdfs/file8.pdf"
    }
}

# Serve static files (HTML, CSS, JS)
app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")

# ì „ì—­ ë³€ìˆ˜ë¡œ LangChain êµ¬ì„±ìš”ì†Œ ì„ ì–¸
retriever = None
llm = None
rag_chain = None

# 1. ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ ì¿¼ë¦¬ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸
query_transformation_prompt = PromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ë¬¸ì„œ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , ê´€ë ¨ ë¬¸ì„œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ 
    ìµœì ì˜ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
    
    ì›ë˜ ì§ˆë¬¸: {question}
    
    ê²€ìƒ‰ì— ì‚¬ìš©í•  í‚¤ì›Œë“œë‚˜ ë¬¸êµ¬ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.
    """
)

# 2. ë¬¸ì„œ ë¶„ì„ í”„ë¡¬í”„íŠ¸
document_analysis_prompt = PromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì •ì±… ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ë¶„ì„í•˜ê³ , ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ì±…ë“¤ì„ ì°¾ì•„ ì •ë¦¬í•´ì£¼ì„¸ìš”.
    
    ì§ˆë¬¸: {question}
    
    ë¬¸ì„œ ë‚´ìš©:
    {context}
    
    ìœ„ ë¬¸ì„œì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê°€ì¥ ì¤‘ìš”í•œ ì •ì±…/ê³µì•½ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
    
    # ì¤‘ìš” ì§€ì‹œì‚¬í•­
    - ë§Œì•½ ì§ˆë¬¸ì´ ì´ì „ì— ì–¸ê¸‰ëœ íŠ¹ì • ë²ˆí˜¸ í•­ëª©(ì˜ˆ: "3ë²ˆ", "4ë²ˆ í•­ëª©")ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…ì„ ìš”ì²­í•˜ëŠ” ê²½ìš°,
      ë‹¤ë¥¸ í•­ëª©ì„ ëª¨ë‘ ë¬´ì‹œí•˜ê³  í•´ë‹¹ í•­ëª©ë§Œ ì‹¬ì¸µì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    - í•­ëª© ì„¤ëª… ìš”ì²­ì¼ ê²½ìš°, ë°˜ë“œì‹œ í•´ë‹¹ í•­ëª©ì˜ í‚¤ì›Œë“œ("ì²­ë…„ê³ ìš©ë³´í—˜", "êµ¬ì§ê¸‰ì—¬" ë“±)ë¥¼ í™œìš©í•˜ì—¬ ë¬¸ì„œ ë‚´ìš©ì„ ê²€ìƒ‰í•˜ê³ 
      ê´€ë ¨ëœ ëª¨ë“  ì„¸ë¶€ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    - í•­ëª© ì„¤ëª… ìš”ì²­ì¼ ê²½ìš°, ì¼ë°˜ì ì¸ ëª©ë¡ í˜•ì‹ì´ ì•„ë‹Œ ìƒì„¸í•œ ì„¤ëª… í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    
    # ì¼ë°˜ ì •ì±… ì§ˆë¬¸ì¸ ê²½ìš° ê·œì¹™
    1. ì •ì±…/ê³µì•½ì€ ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ ì¶”ì¶œí•˜ì„¸ìš”. ì ˆëŒ€ë¡œ 10ê°œë¥¼ ë„˜ê¸°ì§€ ë§ˆì„¸ìš”.
    2. ì¶”ì¶œí•œ ì •ì±…ì˜ ìˆ˜ê°€ 10ê°œ ë¯¸ë§Œì´ë¼ë„ ë¬´ë¦¬í•˜ê²Œ ì±„ìš°ì§€ ë§ˆì„¸ìš”.
    3. ê° ì •ì±…ì€ í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
    4. ê° ì •ì±…ì€ ë°˜ë“œì‹œ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ êµ¬ë¶„í•˜ê³ (1. 2. 3. ë“±), ì •ì±…ë³„ë¡œ í•œ ì¤„ì”© ë„ì›Œì£¼ì„¸ìš”.
    5. ë§ˆì§€ë§‰ì— ì¶œì²˜ê°€ ë˜ëŠ” ê³µì•½ì˜ pageì™€ ë¬¸ì„œëª…ì„ ë°˜ë“œì‹œ í‘œì‹œí•˜ì„¸ìš”.
    6. ê´€ë ¨ ì •ì±…ì´ ì—†ë‹¤ë©´ "ê´€ë ¨ ì •ì±… ì •ë³´ ì—†ìŒ"ì´ë¼ê³ ë§Œ ë‹µí•˜ì„¸ìš”.
    
    ë‹µë³€:
    """
)

# ê°ì • ë¶„ì„ í”„ë¡¬í”„íŠ¸
sentiment_analysis_prompt = PromptTemplate.from_template(
    """ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ê³¼ ì˜ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
    
    í…ìŠ¤íŠ¸: {text}
    
    ì´ í…ìŠ¤íŠ¸ê°€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¼ë„ ëª…í™•í•˜ê²Œ í¬í•¨í•˜ëŠ” ê²½ìš°ì—ë§Œ "ë¶€ì ì ˆ"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”:
    1. ì§ì ‘ì ì¸ ë¹„ì†ì–´ë‚˜ ìš•ì„¤
    2. ëª…ë°±íˆ ì„±ì ìœ¼ë¡œ ë¶€ì ì ˆí•œ ë‚´ìš©
    3. íŠ¹ì • ì§‘ë‹¨ì„ í–¥í•œ í˜ì˜¤ í‘œí˜„ì´ë‚˜ ì°¨ë³„ì  ì–¸ì–´
    4. ì§ì ‘ì ì¸ ìœ„í˜‘ì´ë‚˜ í­ë ¥ì ì¸ ë‚´ìš©
    5. ê°œì¸ì •ë³´ ìš”ì²­ì´ë‚˜ ìœ ì¶œ
    6. ëª…ë°±íˆ ë¶ˆë²•ì ì¸ í™œë™ ìœ ë„
    7. íŠ¹ì • ì •ì¹˜ì¸ì´ë‚˜ ê°œì¸ì— ëŒ€í•œ ì‹¬í•œ ë¹„ë°©ì´ë‚˜ ì¸ì‹ ê³µê²©
    
    ì¼ë°˜ì ì¸ ì§ˆë¬¸, ì •ì±… ë¬¸ì˜, ì¤‘ë¦½ì  ì˜ê²¬, ë‹¨ìˆœí•œ ë¶€ì •ì  ì˜ê²¬ í‘œí˜„ ë“±ì€ "ì ì ˆ"ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    ì •ì¹˜ì  ì£¼ì œë‚˜ ë¹„íŒì  ì§ˆë¬¸ì´ë¼ë„ ì˜ˆì˜ë¥¼ ê°–ì¶”ê³  ìˆë‹¤ë©´ "ì ì ˆ"ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    ì˜ë„ê°€ ë¶ˆë¶„ëª…í•˜ê±°ë‚˜ ë§¥ë½ì´ ë¶ˆì¶©ë¶„í•˜ë‹¤ë©´ "ì ì ˆ"ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    
    ê²°ê³¼(ì •í™•íˆ "ì ì ˆ" ë˜ëŠ” "ë¶€ì ì ˆ" ì¤‘ í•˜ë‚˜ë§Œ ë‹µë³€):
    """
)

# ë¡œê¹… ì„¤ì • í•¨ìˆ˜
def setup_logging():
    """ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = os.path.join(BASE_DIR, "logs")
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    # ë¡œê·¸ íŒŒì¼ ì´ë¦„ (ë‚ ì§œ í¬í•¨)
    log_filename = os.path.join(log_dir, f"chatbot_{datetime.now().strftime('%Y-%m-%d')}.log")
    
    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger("chatbot")
    logger.setLevel(logging.INFO)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ ì„¤ì • (10MB í¬ê¸° ì œí•œ, ìµœëŒ€ 5ê°œ ë°±ì—… íŒŒì¼)
    file_handler = RotatingFileHandler(
        log_filename, 
        maxBytes=10*1024*1024,  # 10 MB
        backupCount=5,
        encoding='utf-8'
    )
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬ ì„¤ì •
    console_handler = logging.StreamHandler()
    
    # í¬ë§· ì„¤ì •
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    # í•¸ë“¤ëŸ¬ ì¶”ê°€
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

# ê°ì • ë¶„ì„ ì²´ì¸ ìƒì„±
def create_sentiment_analysis_chain(llm):
    sentiment_chain = (
        sentiment_analysis_prompt
        | llm
        | StrOutputParser()
    )
    return sentiment_chain

# ê°ì • ë¶„ì„ í•¨ìˆ˜
async def analyze_sentiment(text, sentiment_chain):
    """í…ìŠ¤íŠ¸ì˜ ê°ì •ê³¼ ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆì„± íŒë‹¨"""
    try:
        # ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        result = sentiment_chain.invoke({"text": text})
        
        # ë¡œê·¸ì— ë¶„ì„ ê²°ê³¼ ê¸°ë¡
        logger.info(f"ê°ì • ë¶„ì„ ì›ë³¸ ê²°ê³¼: '{result}' (í…ìŠ¤íŠ¸: '{text[:50]}...')")
        
        # ë‹¨ìˆœí™”ëœ ê²°ê³¼ ì¶”ì¶œ: "ì ì ˆ" ë˜ëŠ” "ë¶€ì ì ˆ" í‚¤ì›Œë“œë¥¼ ì°¾ìŒ
        is_inappropriate = False
        
        # ê²°ê³¼ì—ì„œ ë§ˆì§€ë§‰ 50ìë§Œ ê²€ì‚¬ (ìµœì¢… íŒë‹¨ì€ ë³´í†µ ë§ˆì§€ë§‰ì— ìˆìŒ)
        last_part = result[-10:] if len(result) > 10 else result
        
        if "ë¶€ì ì ˆ" in last_part:
            is_inappropriate = True
        elif "ì ì ˆ" in last_part:
            is_inappropriate = False
        else:
            # ë‘ í‚¤ì›Œë“œê°€ ëª¨ë‘ ì—†ìœ¼ë©´ ë¶„ì„ ì‹¤íŒ¨ë¡œ ê°„ì£¼, ê¸°ë³¸ê°’ ì‚¬ìš©
            logger.warning(f"ê°ì • ë¶„ì„ ê²°ê³¼ì—ì„œ íŒë‹¨ í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {last_part}")
            is_inappropriate = False
        
        logger.info(f"ë¶€ì ì ˆ ì—¬ë¶€ íŒë‹¨: {is_inappropriate}")
        
        return is_inappropriate, result
    except Exception as e:
        logger.error(f"ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False, f"ì˜¤ë¥˜: {str(e)}"
    
# ê¸ˆì§€ì–´ ëª©ë¡
prohibited_words = [
    "ì”¨ë°œ", "ë³‘ì‹ ", "ê°œìƒˆë¼", "ì§€ë„", "ì¢†", "ë‹ˆë¯¸", "fuck", "sex", "bastard", "bitch",
    "ê°œìì‹", "ê±¸ë ˆ", "ì°½ë…€", "ìŒë†ˆ", "ìŒë…„", "ì• ë¯¸", "ì• ë¹„", 
    "ì „í™”ë²ˆí˜¸", "ì£¼ë¯¼ë²ˆí˜¸", "ê³„ì¢Œë²ˆí˜¸", "ì‹ ìš©ì¹´ë“œ", "ë²”ì£„ì", "ì“°ë ˆê¸°"
]

def contains_prohibited_content(text):
    """ì…ë ¥ëœ í…ìŠ¤íŠ¸ì— ë¹„ì†ì–´ë‚˜ ê°œì¸ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    text_lower = text.lower()
    
    # ê¸ˆì§€ì–´ ì²´í¬
    for word in prohibited_words:
        if word.lower() in text_lower:
            return True
            
    # ì´ë¦„ íŒ¨í„´ ì²´í¬
    name_pattern = re.compile(r'[ê°€-í£]{2,4}\s?ì”¨')
    if name_pattern.search(text):
        return True
        
    return False

# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ ë”•ì…”ë„ˆë¦¬
conversation_history = {}

# ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ë©”ì‹œì§€ ì¶”ê°€ í•¨ìˆ˜
def add_to_history(session_id, role, content):
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ë©”ì‹œì§€ ì¶”ê°€"""
    if session_id not in conversation_history:
        conversation_history[session_id] = []
    
    conversation_history[session_id].append({
        "role": role,
        "content": content,
        "timestamp": datetime.now().isoformat()  # datetime ì§ì ‘ ì‚¬ìš©
    })
    
    # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ (ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€)
    if len(conversation_history[session_id]) > 10:
        conversation_history[session_id] = conversation_history[session_id][-10:]

# ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
def get_conversation_context(session_id):
    """ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ìƒì„± - ë‚´ë¶€ ì²˜ë¦¬ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ê³  ì‘ë‹µì—ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ"""
    if session_id not in conversation_history or len(conversation_history[session_id]) == 0:
        return ""
    
    # ìµœê·¼ ë©”ì‹œì§€ 2ìŒ(ì§ˆë¬¸+ë‹µë³€)ë§Œ ì‚¬ìš©
    recent_messages = conversation_history[session_id][-4:] if len(conversation_history[session_id]) >= 4 else conversation_history[session_id]
    
    # ë‚´ë¶€ ì²˜ë¦¬ìš© ì»¨í…ìŠ¤íŠ¸ ìƒì„± (í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬ë˜ì§€ë§Œ ì‚¬ìš©ìì—ê²ŒëŠ” í‘œì‹œë˜ì§€ ì•ŠìŒ)
    context = ""
    for message in recent_messages:
        role_text = "USER" if message["role"] == "user" else "BOT"
        context += f"{role_text}: {message['content']}\n"
    
    return context

# ì „ì²´ ë©€í‹°ëª¨ë‹¬ RAG ì²´ì¸ êµ¬ì„±
def create_multimodal_rag_chain(retriever, llm):
    # 1. ì¿¼ë¦¬ ë³€í™˜ ì²´ì¸
    query_transformer_chain = (
        {"question": RunnablePassthrough()}
        | query_transformation_prompt
        | llm
        | StrOutputParser()
    )
    
    # 2. ê²€ìƒ‰ ì²´ì¸ (ë³€í™˜ëœ ì¿¼ë¦¬ ì‚¬ìš©)
    def retrieve_documents(input_dict):
        original_question = input_dict["original_question"]
        optimized_query = input_dict["optimized_query"]
        
        logger.info(f"ì›ë³¸ ì§ˆë¬¸: {original_question}")
        logger.info(f"ìµœì í™”ëœ ì¿¼ë¦¬: {optimized_query}")
        
        # íŠ¹ì • í•­ëª©ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª… ìš”ì²­ì¸ì§€ í™•ì¸
        item_request_match = re.search(r'([0-9]+)ë²ˆì§¸|([0-9]+)ë²ˆ|([0-9]+)í•­ëª©|([0-9]+)ë²ˆ í•­ëª©', original_question)
        is_item_request = bool(item_request_match)
        
        # íŠ¹ì • í•­ëª© ì„¤ëª… ìš”ì²­ì¼ ê²½ìš° ê²€ìƒ‰ ë°©ì‹ ë³€ê²½
        if is_item_request:
            # ì´ì „ ëŒ€í™”ì—ì„œ í•­ëª© ë‚´ìš© ì°¾ê¸°
            if "BOT:" in original_question:
                # ëŒ€í™” ë‚´ìš©ì—ì„œ ì´ì „ BOT ì‘ë‹µ ì¶”ì¶œ
                bot_response = original_question.split("BOT:")[1].strip()
                
                # ìš”ì²­ëœ í•­ëª© ë²ˆí˜¸ ì¶”ì¶œ
                item_num = next(g for g in item_request_match.groups() if g is not None)
                item_num = int(item_num)
                
                # í•´ë‹¹ í•­ëª©ì˜ ë‚´ìš© ì¶”ì¶œ
                items_pattern = r'([0-9]+)\.\s+(.+?)(?=\n\s*[0-9]+\.|$)'
                items = re.findall(items_pattern, bot_response, re.DOTALL)
                
                if items and 0 < item_num <= len(items):
                    # í•­ëª© ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                    item_content = items[item_num-1][1].strip()
                    logger.info(f"ìƒì„¸ ì„¤ëª… ìš”ì²­ëœ í•­ëª© {item_num}ë²ˆ ë‚´ìš©: {item_content}")
                    
                    # í•­ëª© ë‚´ìš©ì„ í‚¤ì›Œë“œë¡œ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê°•í™”
                    extra_keywords = re.sub(r'\([^)]*\)', '', item_content)  # ê´„í˜¸ ë‚´ìš© ì œê±°
                    keywords = ' '.join(re.findall(r'\b\w+\b', extra_keywords))
                    
                    # ìµœì í™”ëœ ì¿¼ë¦¬ì™€ í•­ëª© í‚¤ì›Œë“œ ê²°í•©
                    enhanced_query = f"{optimized_query} {keywords}"
                    logger.info(f"í•­ëª© ë‚´ìš© ê¸°ë°˜ ê°•í™”ëœ ì¿¼ë¦¬: {enhanced_query}")
                    
                    # ê°•í™”ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
                    retrieved_docs = retriever.invoke(enhanced_query)
                else:
                    # í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆì„ ê²½ìš° ê¸°ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
                    retrieved_docs = retriever.invoke(optimized_query)
            else:
                # ì´ì „ ëŒ€í™” ë‚´ìš©ì´ ì—†ì„ ê²½ìš° ê¸°ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
                retrieved_docs = retriever.invoke(optimized_query)
        else:
            # ì¼ë°˜ ì§ˆë¬¸ì¼ ê²½ìš° ê¸°ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
            retrieved_docs = retriever.invoke(optimized_query)
        
        logger.info(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}")
        if retrieved_docs:
            logger.info(f"ì²« ë²ˆì§¸ ë¬¸ì„œ ì¼ë¶€: {retrieved_docs[0].page_content[:100]}...")
        
        # íŠ¹ì • í•­ëª© ì„¤ëª… ìš”ì²­ì¸ì§€ ì—¬ë¶€ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨
        return {
            "question": original_question,
            "context": "\n\n".join([doc.page_content for doc in retrieved_docs]),
            "is_item_request": is_item_request,
            "item_number": item_num if is_item_request and 'item_num' in locals() else None
        }
    
    # 3. ë¬¸ì„œ ë¶„ì„ ì²´ì¸
    document_analysis_chain = (
        document_analysis_prompt
        | llm
        | StrOutputParser()
    )
    
    # ì „ì²´ ë©€í‹°ëª¨ë‹¬ ì²´ì¸ êµ¬ì„±
    multimodal_chain = (
        # 1ë‹¨ê³„: ì›ë³¸ ì§ˆë¬¸ ì €ì¥ ë° ì¿¼ë¦¬ ìµœì í™”
        RunnablePassthrough().with_config(run_name="Original Question")
        | {"original_question": lambda x: x, "optimized_query": query_transformer_chain}
        
        # 2ë‹¨ê³„: ìµœì í™”ëœ ì¿¼ë¦¬ë¡œ ë¬¸ì„œ ê²€ìƒ‰
        | RunnableLambda(retrieve_documents).with_config(run_name="Document Retrieval")
        
        # 3ë‹¨ê³„: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¶„ì„
        | {"question": lambda x: x["question"], 
           "analyzed_info": document_analysis_chain}
        
        # 4ë‹¨ê³„: ìµœì¢… ì‘ë‹µ ìƒì„± (ë¶„ì„ëœ ì •ë³´ë¥¼ ì‚¬ìš©ì ì¹œí™”ì  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜)
        | RunnableLambda(lambda x: format_response(x["question"], x["analyzed_info"]))
    )
    
    return multimodal_chain

# format_response í•¨ìˆ˜ì— ë¡œê·¸ ì¶”ê°€
def format_response(question, analyzed_info):
    """ì‘ë‹µì„ ì‚¬ìš©ì ì¹œí™”ì ì¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    logger.info(f"ì›ë³¸ ì§ˆë¬¸: {question}")
    logger.info(f"ë¶„ì„ëœ ì •ë³´: {analyzed_info}")
    
    # ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìƒˆ ì§ˆë¬¸ ë¶„ë¦¬
    clean_question = question
    if "ìƒˆë¡œìš´ ì§ˆë¬¸:" in question:
        clean_question = question.split("ìƒˆë¡œìš´ ì§ˆë¬¸:")[-1].strip()
    
    # ì°¸ì¡° í•­ëª© ì •ë³´ ì œê±°
    if "(ì°¸ì¡° í•­ëª©:" in clean_question:
        clean_question = clean_question.split("(ì°¸ì¡° í•­ëª©:")[0].strip()
    
    logger.info(f"ì •ì œëœ ì§ˆë¬¸: {clean_question}")
    
    # ì •ì±… ìë£Œ ë‹¤ìš´ë¡œë“œ ìš”ì²­ì¸ì§€ í™•ì¸
    is_download_request = any(keyword in clean_question.lower() for keyword in 
                            ["ìë£Œ", "ë‹¤ìš´ë¡œë“œ", "íŒŒì¼", "ë°›ê¸°", "pdf", "ì •ì±…ì§‘", "ê³µì•½ì§‘"])
    
    # ì •ì±… ìë£Œ ë‹¤ìš´ë¡œë“œ ìš”ì²­ì¸ ê²½ìš° ë‹¤ìš´ë¡œë“œ ë§í¬ ì œê³µ
    if is_download_request:
        download_links = (
                "ğŸ¤– ì •ì±… ìë£Œë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆëŠ” ë§í¬ë¥¼ ì œê³µí•´ ë“œë¦½ë‹ˆë‹¤:<br><br>"
                "<div style='display: flex; gap: 10px; flex-wrap: wrap;'>"
                "<a href='/static/pdfs/full.pdf' style='padding: 10px 20px; background-color: #0078d4; color: white; text-decoration: none; border-radius: 4px; margin-bottom: 10px;' target='_blank'>ì •ì±…ê³µì•½ì§‘ ë‹¤ìš´ë¡œë“œ</a>"
                "<a href='/static/pdfs/region_document.pdf' style='padding: 10px 20px; background-color: #0078d4; color: white; text-decoration: none; border-radius: 4px; margin-bottom: 10px;' target='_blank'>ì§€ì—­ê³µì•½ì§‘ ë‹¤ìš´ë¡œë“œ</a>"
                "</div><br>"
                "ìœ„ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ PDF íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê±°ë‚˜ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br><br>"
                "ì„¸ë¶€ ìë£Œë¥¼ í™•ì¸í•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ ê³µì•½ì •ì±… í˜ì´ì§€ë¡œ ì´ë™í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:<br><br>"
                "<a href='/pdf' style='padding: 10px 20px; background-color: #5cb85c; color: white; text-decoration: none; border-radius: 4px; display: inline-block;'>ê³µì•½ì •ì±… í˜ì´ì§€ ì´ë™</a><br><br>"
                "ì¶”ê°€ë¡œ ê¶ê¸ˆí•˜ì‹  ë‚´ìš©ì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”."
            )
        return download_links
    
    # ì‘ë‹µì— "ê´€ë ¨ ì •ì±… ì •ë³´ ì—†ìŒ"ì´ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨ëœ ê²½ìš°ì—ë§Œ ì •ë³´ ì—†ìŒìœ¼ë¡œ ì²˜ë¦¬
    if "ê´€ë ¨ ì •ì±… ì •ë³´ ì—†ìŒ" in analyzed_info[-30:]:
        logger.info(f"ê´€ë ¨ ì •ì±… ì •ë³´ê°€ ì—†ìŒ analyzed_info : {analyzed_info}")
        logger.info(f" analyzed_info[-30:] : {analyzed_info[-30:]}")
        return f"ğŸ¤– {clean_question}ì— ê´€í•œ ì •ì±… ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ìœ¼ë¡œ ì‹œë„í•´ ë³´ì„¸ìš”."
    
    # 'ë‹µë³€:' ì´í›„ ë‚´ìš©ë§Œ ì¶”ì¶œ
    final_answer = analyzed_info
    if "ë‹µë³€:" in analyzed_info:
        final_answer = analyzed_info.split("ë‹µë³€:")[1].strip()
    
    # íŠ¹ì • í•­ëª© ì„¤ëª… ìš”ì²­ì¸ì§€ í™•ì¸
    is_item_request = bool(re.search(r'([0-9]+)ë²ˆì§¸|([0-9]+)ë²ˆ|([0-9]+)í•­ëª©|([0-9]+)ë²ˆ í•­ëª©', clean_question))
    
    # íŠ¹ì • í•­ëª© ì„¤ëª… ìš”ì²­ì¸ ê²½ìš° ì‘ë‹µ í˜•ì‹ ì¡°ì •
    if is_item_request:
        item_match = re.search(r'([0-9]+)ë²ˆì§¸|([0-9]+)ë²ˆ|([0-9]+)í•­ëª©|([0-9]+)ë²ˆ í•­ëª©', clean_question)
        item_num = next(g for g in item_match.groups() if g is not None)
        response = f"ğŸ¤– {item_num}ë²ˆ í•­ëª©ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì…ë‹ˆë‹¤.\n\n{final_answer}"
    else:
        # ì¼ë°˜ ì‘ë‹µì¸ ê²½ìš° ê¸°ë³¸ í˜•ì‹ ì‚¬ìš©
        response = f"ğŸ¤– {clean_question} ê´€ë ¨ ë‹µë³€ë“œë¦½ë‹ˆë‹¤.\n\n{final_answer}"
    
    # ë¡œê·¸ì— ìµœì¢… ì‘ë‹µ ê¸°ë¡
    logger.info(f"ìµœì¢… ì‘ë‹µ: {response}...")
    
    return response

def prepare_contextual_message(user_message, session_id):
    """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì™€ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ì¿¼ë¦¬ ìƒì„±"""
    # 1. ì»¨í…ìŠ¤íŠ¸ ë° ì°¸ì¡° í•­ëª© í™•ì¸
    query, context_info = process_query_with_context(user_message, session_id)
    
    # 2. ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ í™•ì¸ (ìµœëŒ€ 2ê°œ ì§ˆì˜ì‘ë‹µ ìŒ)
    previous_context = ""
    
    if session_id in conversation_history and len(conversation_history[session_id]) > 0:
        # ìµœê·¼ 2ìŒì˜ ëŒ€í™”ë§Œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
        recent_msgs = []
        qa_pairs = 0
        for msg in reversed(conversation_history[session_id]):
            recent_msgs.append(msg)
            if msg["role"] == "user":
                qa_pairs += 1
                if qa_pairs >= 2:  # ìµœëŒ€ 2ê°œ ì§ˆì˜ì‘ë‹µ ìŒë§Œ ì‚¬ìš©
                    break
        
        # ë‹¤ì‹œ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì¬ì •ë ¬
        recent_msgs.reverse()
        
        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        for msg in recent_msgs:
            role_text = "USER" if msg["role"] == "user" else "BOT"
            # ì±—ë´‡ ì‘ë‹µì—ì„œ í—¤ë” ì œê±° (ğŸ¤– ... ê´€ë ¨ ë‹µë³€ë“œë¦½ë‹ˆë‹¤ ë¶€ë¶„)
            if role_text == "BOT" and "ê´€ë ¨ ë‹µë³€ë“œë¦½ë‹ˆë‹¤" in msg["content"]:
                content = msg["content"].split("ê´€ë ¨ ë‹µë³€ë“œë¦½ë‹ˆë‹¤", 1)[1].strip()
            else:
                content = msg["content"]
            
            previous_context += f"{role_text}: {content}\n"
    
    # 3. ìµœì¢… ì¿¼ë¦¬ êµ¬ì„±
    # í•­ëª© ìì„¸íˆ ì„¤ëª… ìš”ì²­ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
    if context_info and context_info.get("request_type") == "item_detail":
        final_query = (
            f"{previous_context}\n"
            f"ìƒˆë¡œìš´ ì§ˆë¬¸: {user_message}\n"
            f"ì´ì „ ì§ˆë¬¸: {context_info.get('previous_question', '')}\n"
            f"ì„¤ëª…í•  í•­ëª© ë²ˆí˜¸: {context_info.get('item_number')}\n"
            f"ì„¤ëª…í•  í•­ëª© ë‚´ìš©: {context_info.get('selected_item')}\n"
            f"ìš”ì²­ ìœ í˜•: íŠ¹ì • í•­ëª©ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª… ìš”ì²­"
        )
    # ì¼ë°˜ ì°¸ì¡° í•­ëª©ì´ ìˆì„ ê²½ìš° ì¶”ê°€ ì •ë³´ë¡œ ì œê³µ
    elif context_info:
        final_query = (
            f"{previous_context}\n"
            f"ìƒˆë¡œìš´ ì§ˆë¬¸: {user_message}\n"
            f"ì°¸ì¡° í•­ëª©: {context_info}"
        )
    else:
        final_query = f"{previous_context}\nìƒˆë¡œìš´ ì§ˆë¬¸: {user_message}"
    
    logger.info(f"ìµœì¢… ì¿¼ë¦¬: {final_query[:200]}..." if len(final_query) > 200 else f"ìµœì¢… ì¿¼ë¦¬: {final_query}")
    
    return final_query

# ê°œì„ ëœ ì§ˆì˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜
def process_query_with_context(query, session_id):
    """ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ ì§ˆì˜ ì²˜ë¦¬"""
    if session_id not in conversation_history or len(conversation_history[session_id]) == 0:
        return query, None
    
    # ìµœê·¼ ì±—ë´‡ ì‘ë‹µ ì°¾ê¸°
    latest_bot_response = None
    previous_question = None
    for msg in reversed(conversation_history[session_id]):
        if msg["role"] == "assistant" and not latest_bot_response:
            latest_bot_response = msg["content"]
        elif msg["role"] == "user" and previous_question is None and latest_bot_response:
            previous_question = msg["content"]
            break
    
    if not latest_bot_response:
        return query, None
    
    # í•­ëª© ë²ˆí˜¸ ì¶”ì¶œ ì‹œë„
    item_match = re.search(r'([0-9]+)ë²ˆì§¸|([0-9]+)ë²ˆ|([0-9]+)í•­ëª©|([0-9]+)ë²ˆ í•­ëª©', query)
    if item_match:
        # ìˆ«ì ì¶”ì¶œ
        item_num = next(g for g in item_match.groups() if g is not None)
        item_num = int(item_num)
        logger.info(f"í•­ëª© ë²ˆí˜¸ ì¶”ì¶œë¨: {item_num}")
        
        # ì´ì „ ì‘ë‹µì—ì„œ í•­ëª© íŒ¨í„´ ì¶”ì¶œ
        try:
            # í•­ëª© íŒ¨í„´ (ìˆ«ì. ë‚´ìš©) ì°¾ê¸°
            items = re.findall(r'([0-9]+)\.\s+(.+?)(?=\n\n[0-9]+\.|$)', latest_bot_response, re.DOTALL)
            if items and 0 < item_num <= len(items):
                # ì°¾ì€ í•­ëª©ì˜ ë‚´ìš©
                item_content = items[item_num - 1][1].strip()
                logger.info(f"ì°¾ì€ í•­ëª© ë‚´ìš©: {item_content}")
                
                # ì‘ë‹µì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± - ì´ì „ ì§ˆë¬¸ê³¼ í•­ëª© ì •ë³´ í¬í•¨
                context_info = {
                    "previous_question": previous_question,
                    "selected_item": item_content,
                    "item_number": item_num,
                    "request_type": "item_detail"
                }
                
                return query, context_info
        except Exception as e:
            logger.error(f"í•­ëª© ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
    
    return query, None

# init_rag_system í•¨ìˆ˜ì— ë¡œê·¸ ì¶”ê°€
'''
def init_rag_system():
    """LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global retriever, llm, rag_chain ,sentiment_chain

    logger.info("LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

    # 1. PDF ë¬¸ì„œ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
    documents = []
    for pdf_path in pdf_paths:
        if os.path.exists(pdf_path):
            logger.info(f"PDF íŒŒì¼ ë¡œë“œ ì¤‘: {pdf_path}")
            loader = PyPDFLoader(pdf_path)
            docs = loader.load()
            documents.extend(docs)
            logger.info(f"- {len(docs)}ê°œ í˜ì´ì§€ ë¡œë“œë¨")
        else:
            logger.info(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_path}")

    if not documents:
        logger.info("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False
    
    logger.info(f"ì´ {len(documents)}ê°œ í˜ì´ì§€ ë¡œë“œë¨")

    # 2. í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_documents(documents)
    logger.info(f"ë¬¸ì„œë¥¼ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    # 3. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
    logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/distiluse-base-multilingual-cased-v1",
        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")

    # 4. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
    logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...")
    vectorstore = FAISS.from_documents(chunks, embedding_model)
    logger.info("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ")

    # ìƒ˜í”Œ ë¬¸ì„œ í™•ì¸ (ë””ë²„ê¹…ìš©)
    for i, doc_id in enumerate(list(vectorstore.docstore._dict.keys())[:3]):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
        logger.info(f"ë¬¸ì„œ ID {doc_id}ì˜ ë‚´ìš©:")
        logger.info(vectorstore.docstore._dict[doc_id])
        logger.info("-" * 50)
        if i >= 2:  # ìµœëŒ€ 3ê°œë§Œ ì¶œë ¥
            break

    # 5. ê²€ìƒ‰ê¸°(Retriever) ì„¤ì • - ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì§ì ‘ ìƒì„±
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 5}
    )

    # 6. EXAONE ëª¨ë¸ ë¡œë“œ ë° LangChain LLM ë˜í¼ ì„¤ì •
    model_name = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"
    #model_name = "LGAI-EXAONE/EXAONE-Deep-32B-AWQ"
    #streaming = True 
    logger.info(f"{model_name} ëª¨ë¸ ë¡œë“œ ì¤‘...")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )

    # ì–‘ìí™” ì„¤ì •ì„ BitsAndBytesConfigë¡œ êµ¬ì„±
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True
    )

    # ëª¨ë¸ ë¡œë“œ - ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )

    # ëª¨ë¸ ìµœì í™” ì„¤ì • ì ìš©
    optimize_performance(model)

    # ì§ì ‘ Hugging Face íŒŒì´í”„ë¼ì¸ ìƒì„± í›„ LangChain ë˜í¼ ì ìš©
    from transformers import pipeline

    # íŠ¸ëœìŠ¤í¬ë¨¸ íŒŒì´í”„ë¼ì¸ ìƒì„±
    hf_pipeline = pipeline(
        task="text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=300,
        do_sample=True,
        temperature=0.3,
        device_map="auto",
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id
    )

    # LangChain HuggingFacePipeline ë˜í¼ ìƒì„±
    llm = HuggingFacePipeline(pipeline=hf_pipeline)

    logger.info("EXAONE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    rag_chain = create_multimodal_rag_chain(retriever, llm)

    # ê°ì • ë¶„ì„ ì²´ì¸ ìƒì„±
    sentiment_chain = create_sentiment_analysis_chain(llm)

    logger.info("LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    return True
'''

# ëª¨ë¸ ë¡œë“œ ë° í† í¬ë‚˜ì´ì € ì„¤ì • ë¶€ë¶„
def init_rag_system():
    """LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global retriever, llm, rag_chain, sentiment_chain, model, tokenizer

    logger.info("LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

    # 1-4. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ë¶€ë¶„ (ì½”ë“œ ìœ ì§€)...
    
    # 5. EXAONE ëª¨ë¸ ë¡œë“œ ë¶€ë¶„ ìˆ˜ì •
    model_name = "LGAI-EXAONE/EXAONE-Deep-32B-AWQ"  # 32B ëª¨ë¸ë¡œ ë³€ê²½
    logger.info(f"{model_name} ëª¨ë¸ ë¡œë“œ ì¤‘...")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )

    # ëª¨ë¸ ë¡œë“œ - ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,  # ë˜ëŠ” torch.float16
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )

    # ëª¨ë¸ ìµœì í™” ì„¤ì • ì ìš©
    optimize_performance(model)

    # LangChain ìš© ì»¤ìŠ¤í…€ LLM í´ë˜ìŠ¤ ì •ì˜
    from langchain.llms.base import LLM
    from typing import Any, List, Mapping, Optional
    
    class CustomEXAONELLM(LLM):
        model: Any
        tokenizer: Any
        
        def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
            messages = [{"role": "user", "content": prompt}]
            input_ids = self.tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="pt"
            ).to(model.device)
            
            with torch.no_grad():
                output = self.model.generate(
                    input_ids,
                    eos_token_id=self.tokenizer.eos_token_id,
                    max_new_tokens=800,  # ë” ë§ì€ í† í° ìƒì„±
                    do_sample=True,
                    temperature=0.3,
                    top_p=0.92,
                    repetition_penalty=1.2,  # ë°˜ë³µ ë°©ì§€
                )
            
            response = self.tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True)
            
            # ì‘ë‹µì´ ì˜ë¦¬ëŠ” ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ í›„ì²˜ë¦¬
            response = self._clean_response(response)
            
            return response
        
        def _clean_response(self, text: str) -> str:
            """ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ë¦¬"""
            # ë°˜ë³µë˜ëŠ” íŒ¨í„´ ì œê±° ë¡œì§
            pattern = r'1\.\s+'
            matches = list(re.finditer(pattern, text))
            
            if len(matches) > 1:
                # ì²« ë²ˆì§¸ 1ë²ˆ í•­ëª© ì´í›„ì˜ í…ìŠ¤íŠ¸ë§Œ ìœ ì§€
                first_match_pos = matches[0].start()
                second_match_pos = matches[1].start()
                return text[:second_match_pos]
            
            return text
        
        @property
        def _identifying_params(self) -> Mapping[str, Any]:
            return {"model_name": "Custom EXAONE Model"}
        
        @property
        def _llm_type(self) -> str:
            return "custom_exaone"
    
    # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    llm = CustomEXAONELLM(model=model, tokenizer=tokenizer)
    
    # 6. RAG ì²´ì¸ ë° ê°ì • ë¶„ì„ ì²´ì¸ ìƒì„±
    rag_chain = create_multimodal_rag_chain(retriever, llm)
    sentiment_chain = create_sentiment_analysis_chain(llm)

    logger.info("LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    return True

def optimize_performance(model):
    """ì„±ëŠ¥ ìµœì í™” ì„¤ì • ì ìš©"""
    # GPU ë©”ëª¨ë¦¬ ìµœì í™”
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.cuda.empty_cache()
        torch.cuda.set_device(torch.cuda.current_device())

    # ëª¨ë¸ ìµœì í™”
    if hasattr(model, 'config'):
        if hasattr(model.config, 'use_cache'):
            model.config.use_cache = True
        if hasattr(model.config, 'gradient_checkpointing'):
            model.config.gradient_checkpointing = False

    logger.info("ì„±ëŠ¥ ìµœì í™” ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ê¸°ë³¸ ê²½ë¡œ - ë©”ì¸ HTML íŒŒì¼ ë°˜í™˜
@app.get('/')
async def root():
    return FileResponse(os.path.join(STATIC_DIR, "index.html"))

# PDF í˜ì´ì§€ ë¼ìš°íŠ¸ ì¶”ê°€
@app.get('/pdf')
async def pdf_page():
    return FileResponse(os.path.join(STATIC_DIR, "pdf.html"))

@app.get("/view-pdf")
async def view_pdf():
    # StaticFiles ë¯¸ë“¤ì›¨ì–´ê°€ ì´ë¯¸ /static ê²½ë¡œì— ë§ˆìš´íŠ¸ë˜ì–´ ìˆìœ¼ë¯€ë¡œ
    # STATIC_DIR ë‚´ë¶€ì˜ ê²½ë¡œë§Œ ì§€ì •
    file_path = os.path.join(STATIC_DIR, "pdfs/region_document.pdf")
    if not os.path.exists(file_path):
        from fastapi import HTTPException
        raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return FileResponse(
        path=file_path,
        media_type="application/pdf",
        content_disposition_type="inline"
    )

# PDF íŒŒì¼ ì •ë³´ API ì—”ë“œí¬ì¸íŠ¸
@app.get('/api/files')
async def get_files():
    return JSONResponse(pdf_files_info)

# ëŒ“ê¸€ ëª¨ë¸ ì •ì˜
class Comment(BaseModel):
    text: str

# ëŒ“ê¸€ í˜ì´ì§€ ë¼ìš°íŠ¸ - FileResponse ì‚¬ìš©
@app.get("/comments")
async def get_comments_page():
    return FileResponse(os.path.join(BASE_DIR, "comments.html"))

# ëŒ“ê¸€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° API
@app.get("/api/comments")
async def get_comments():
    comments = []
    try:
        with open(CSV_FILE, "r", encoding="utf-8") as file:
            reader = csv.DictReader(file)
            for row in reader:
                comments.append({
                    "id": row["id"],
                    "text": row["text"],
                    "timestamp": row["timestamp"]
                })
        return {"success": True, "comments": comments}
    except Exception as e:
        logger.error(f"ëŒ“ê¸€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ëŒ“ê¸€ ì €ì¥í•˜ê¸° API
@app.post("/api/comments")
async def create_comment(comment: Comment):
    try:
        comment_id = str(uuid.uuid4())
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with open(CSV_FILE, "a", newline="", encoding="utf-8") as file:
            writer = csv.writer(file)
            writer.writerow([comment_id, comment.text, timestamp])
        logger.info(f"ìƒˆë¡œìš´ ëŒ“ê¸€ì´ ì €ì¥ë¨: {comment_id}")
        return {"success": True, "id": comment_id}
    except Exception as e:
        logger.error(f"ëŒ“ê¸€ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ì›¹ ì±„íŒ… API ì—”ë“œí¬ì¸íŠ¸
@app.post('/api/chat')
async def chat_endpoint(request: Request):
    global rag_chain, sentiment_chain
    
    # ìš”ì²­ ë°”ë”” íŒŒì‹±
    req_data = await request.json()
    user_message = req_data.get('message', '')
    session_id = req_data.get('session_id', 'default')
    debug_mode = req_data.get('debug_mode', False)  # ë””ë²„ê·¸ ëª¨ë“œ í”Œë˜ê·¸ ì¶”ê°€
    
    logger.info(f"ì„¸ì…˜ {session_id}ì—ì„œ ìƒˆë¡œìš´ ë©”ì‹œì§€ ìˆ˜ì‹ : {user_message[:30]}..." if len(user_message) > 30 else user_message)
    
    if not user_message:
        logger.warning("ë¹ˆ ë©”ì‹œì§€ê°€ ì „ì†¡ë¨")
        return JSONResponse({"response": "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."})
    
    try:
        sentiment_debug_info = {}  # ê°ì • ë¶„ì„ ë””ë²„ê·¸ ì •ë³´
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ê¸ˆì§€ì–´ í•„í„°ë§
        if contains_prohibited_content(user_message):
            logger.warning(f"ê¸ˆì§€ì–´ í•„í„°ë§ - ë¶€ì ì ˆí•œ ë‚´ìš© ê°ì§€: {user_message[:30]}...")
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. ë¶€ì ì ˆí•œ ì–¸ì–´ë‚˜ ê°œì¸ì •ë³´ê°€ í¬í•¨ëœ ì§ˆë¬¸ì—ëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            sentiment_debug_info["filter_type"] = "prohibited_words"
            
            if debug_mode:
                return JSONResponse({
                    "response": response,
                    "debug_info": sentiment_debug_info
                })
            return JSONResponse({"response": response})
        
        # 2ë‹¨ê³„: ê°ì • ë¶„ì„ì„ í†µí•œ ë¶€ì •ì  ì–´íœ˜ í•„í„°ë§
        is_inappropriate, analysis_result = await analyze_sentiment(user_message, sentiment_chain)
        
        # ë””ë²„ê·¸ ì •ë³´ ì €ì¥
        sentiment_debug_info = {
            "analysis_result": analysis_result,
            "is_inappropriate": is_inappropriate
        }
        
        if is_inappropriate:
            logger.warning(f"ê°ì • ë¶„ì„ í•„í„°ë§ - ë¶€ì •ì  ë‚´ìš© ê°ì§€: {user_message[:30]}...")
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. ë¶€ì ì ˆí•˜ê±°ë‚˜ ë¶€ì •ì ì¸ ë‚´ìš©ì´ í¬í•¨ëœ ì§ˆë¬¸ì—ëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            sentiment_debug_info["filter_type"] = "sentiment_analysis"
            
            if debug_mode:
                return JSONResponse({
                    "response": response,
                    "debug_info": sentiment_debug_info
                })
            return JSONResponse({"response": response})
        
        # 1. ì»¨í…ìŠ¤íŠ¸ ë° ì§ˆì˜ ì¤€ë¹„ (ì´ì „ ëŒ€í™” + ìƒˆ ì§ˆë¬¸)
        contextual_query = prepare_contextual_message(user_message, session_id)
        
        # 2. RAG ì²´ì¸ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        logger.info("RAG ì²´ì¸ìœ¼ë¡œ ì‘ë‹µ ìƒì„± ì¤‘...")
        response = rag_chain.invoke(contextual_query)
        
        # 3. ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì €ì¥ (ì›ë³¸ ì§ˆë¬¸ê³¼ ì‘ë‹µ)
        add_to_history(session_id, "user", user_message)
        add_to_history(session_id, "assistant", response)
        
        return JSONResponse({
            "response": response,
            "session_id": session_id
        })
    except Exception as e:
        logger.error(f"ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        return JSONResponse({
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            "session_id": session_id,
            "error": str(e) if debug_mode else None
        })

# ì„œë²„ ì´ˆê¸°í™” ë° ì‹¤í–‰ì„ ìœ„í•œ ì´ë²¤íŠ¸
@app.on_event("startup")
async def startup_event():
    global logger
    
    # ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •
    logger = setup_logging()
    logger.info("==== ì›¹ ì±—ë´‡ ì„œë²„ ì‹œì‘ ====")
    
    # ëª¨ë¸ ë° RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    logger.info("LangChain RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘")
    init_success = init_rag_system()

    if not init_success:
        logger.error("ì´ˆê¸°í™” ì‹¤íŒ¨. ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        import sys
        sys.exit(1)
    else:
        logger.info("ì„œë²„ ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ! ì›¹ ì±—ë´‡ ì„œë²„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
  

# ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    # 80ë²ˆ í¬íŠ¸ ì‚¬ìš© (root ê¶Œí•œ í•„ìš”)
    #uvicorn.run("chatbot_web:app", host="0.0.0.0", port=80, reload=True)
    
    # ê°œë°œìš©ìœ¼ë¡œëŠ” 5000ë²ˆ í¬íŠ¸ ì‚¬ìš©
    uvicorn.run("chatbot_web:app", host="0.0.0.0", port=5000,http="auto", reload=True)
    # HTTPSë¡œ ì‹¤í–‰
    #uvicorn.run(
    #    "chatbot_web:app", 
    #    host="0.0.0.0", 
    #    port=5000, 
    #    ssl_keyfile=os.path.join(BASE_DIR, "ca/private.pem"),  # ê°œì¸ í‚¤ ê²½ë¡œ 
    #    ssl_certfile=os.path.join(BASE_DIR, "ca/private_ct.pem"),  # ì¸ì¦ì„œ ê²½ë¡œ
    #    reload=True
    #)
